{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Methods are a Leak/Variance Trade-Off\n",
    "\n",
    "  * John Mount, [Win Vector LLC](http://www.win-vector.com/)\n",
    "  * Nina Zumel, [Win Vector LLC](http://www.win-vector.com/)\n",
    "  * March 10, 2020\n",
    "  * [https://github.com/WinVector/pyvtreat/blob/master/Examples/CrossVal/LeakTradeOff/](https://github.com/WinVector/pyvtreat/blob/master/Examples/CrossVal/LeakTradeOff/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Cross-methods such as [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29), and [cross-prediction](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html#sklearn.model_selection.cross_val_predict) are effective tools for many machine learning, statisitics, and data science related applications. They are useful for parameter selection, model selection, impact/target encoding of high cardinality variables, stacking models, and super learning. They are more statistically efficient than partitioning training data into calibration/training/holdout sets, but do not satisfy the full exchangeability conditions that full hold-out methods have. This introduces some additional statistical trade-offs when using cross-methods, beyond the obvious increases in computational cost.\n",
    "\n",
    "Specifically, cross-methods can introduce an information leak into the modeling process. This information leak will be the subject of this post.\n",
    "\n",
    "To show the information leak, we will use a simple artificial problem where there is no relation between the proposed explanatory variable(s) and the output variable. This example is in the spirit of our previous article, [Bad Bayes: an example of why you need hold-out testing](http://www.win-vector.com/blog/2014/02/bad-bayes-an-example-of-why-you-need-hold-out-testing/), as well as the paper by Claudia Perlich Grzegorz Swirszcz, \"On Cross-Validation and Stacking: Building Seemingly Predictive Models On Random Data\", SIGKDD Explorations, volume 12, number 2, 2010.\n",
    "\n",
    "We will demonstrate that even in this situation, target-encoding (or conditionally re-encoding) categorical variables prior to the model-fitting step leaks information about the dependent variable. *This is true even when using cross-methods*. This leaked information may cause the downstream modeling step to treat noise variables as informative ones, leading to overfit.\n",
    "\n",
    "Finally, we will conclude with a more realistic case: a combination of useless and useful explanatory variables.  For our last example we will use our recommended package [`vtreat`](https://github.com/WinVector/pyvtreat) ([available for `Python`](https://github.com/WinVector/pyvtreat) and [available for `R`](https://github.com/WinVector/vtreat)). The `vtreat` package manages impact coding, cross-validation, and reporting in a convenient unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "\n",
    "We will work some regression examples using `Python`/`Pandas`.  In addition to calling out what to look for in each result, we will add \"`assert`\" statements to doublecheck the results as we present them.\n",
    "\n",
    "First we import our packages and modules, and set our pseudo-random state to make the result more easily reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# https://numpy.org\n",
    "import numpy\n",
    "\n",
    "# https://pandas.pydata.org\n",
    "import pandas\n",
    "\n",
    "# https://seaborn.pydata.org\n",
    "import seaborn\n",
    "\n",
    "# https://matplotlib.org\n",
    "import matplotlib.pyplot\n",
    "\n",
    "# https://scikit-learn.org/\n",
    "import sklearn.metrics\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "# https://contrib.scikit-learn.org/categorical-encoding/targetencoder.html\n",
    "import category_encoders\n",
    "\n",
    "# https://www.statsmodels.org/\n",
    "import statsmodels.api\n",
    "\n",
    "# https://github.com/WinVector/pyvtreat/blob/master/Examples/CrossVal/LeakTradeOff/break_cross_val.py\n",
    "# https://github.com/WinVector/data_algebra\n",
    "from break_cross_val import mk_data, TransformerAdapter, Container, solve_for_partition, collect_relations\n",
    "\n",
    "# https://github.com/WinVector/pyvtreat\n",
    "import vtreat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "numpy.random.seed(2020)\n",
    "prng = numpy.random.RandomState(numpy.random.randint(2**32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Example Data: The Pure Noise Case\n",
    "\n",
    "Now we create some example data. The data has 100 rows of 10 categorical variables, with 50 levels each, and one constant variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d_example, y_example = mk_data(\n",
    "    nrow=100,\n",
    "    n_noise_var=10,\n",
    "    n_noise_level=50,\n",
    "    n_signal_var=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const_col</th>\n",
       "      <th>noise_0</th>\n",
       "      <th>noise_1</th>\n",
       "      <th>noise_2</th>\n",
       "      <th>noise_3</th>\n",
       "      <th>noise_4</th>\n",
       "      <th>noise_5</th>\n",
       "      <th>noise_6</th>\n",
       "      <th>noise_7</th>\n",
       "      <th>noise_8</th>\n",
       "      <th>noise_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>nl_14</td>\n",
       "      <td>nl_48</td>\n",
       "      <td>nl_0</td>\n",
       "      <td>nl_36</td>\n",
       "      <td>nl_11</td>\n",
       "      <td>nl_37</td>\n",
       "      <td>nl_14</td>\n",
       "      <td>nl_0</td>\n",
       "      <td>nl_28</td>\n",
       "      <td>nl_41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>nl_4</td>\n",
       "      <td>nl_45</td>\n",
       "      <td>nl_45</td>\n",
       "      <td>nl_6</td>\n",
       "      <td>nl_25</td>\n",
       "      <td>nl_10</td>\n",
       "      <td>nl_18</td>\n",
       "      <td>nl_37</td>\n",
       "      <td>nl_19</td>\n",
       "      <td>nl_38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>nl_36</td>\n",
       "      <td>nl_38</td>\n",
       "      <td>nl_9</td>\n",
       "      <td>nl_34</td>\n",
       "      <td>nl_29</td>\n",
       "      <td>nl_49</td>\n",
       "      <td>nl_18</td>\n",
       "      <td>nl_14</td>\n",
       "      <td>nl_0</td>\n",
       "      <td>nl_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>nl_25</td>\n",
       "      <td>nl_31</td>\n",
       "      <td>nl_18</td>\n",
       "      <td>nl_36</td>\n",
       "      <td>nl_41</td>\n",
       "      <td>nl_30</td>\n",
       "      <td>nl_5</td>\n",
       "      <td>nl_31</td>\n",
       "      <td>nl_21</td>\n",
       "      <td>nl_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>nl_14</td>\n",
       "      <td>nl_24</td>\n",
       "      <td>nl_5</td>\n",
       "      <td>nl_5</td>\n",
       "      <td>nl_1</td>\n",
       "      <td>nl_23</td>\n",
       "      <td>nl_27</td>\n",
       "      <td>nl_42</td>\n",
       "      <td>nl_34</td>\n",
       "      <td>nl_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>a</td>\n",
       "      <td>nl_17</td>\n",
       "      <td>nl_13</td>\n",
       "      <td>nl_33</td>\n",
       "      <td>nl_13</td>\n",
       "      <td>nl_49</td>\n",
       "      <td>nl_31</td>\n",
       "      <td>nl_35</td>\n",
       "      <td>nl_32</td>\n",
       "      <td>nl_17</td>\n",
       "      <td>nl_35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>a</td>\n",
       "      <td>nl_49</td>\n",
       "      <td>nl_43</td>\n",
       "      <td>nl_14</td>\n",
       "      <td>nl_3</td>\n",
       "      <td>nl_32</td>\n",
       "      <td>nl_47</td>\n",
       "      <td>nl_27</td>\n",
       "      <td>nl_23</td>\n",
       "      <td>nl_30</td>\n",
       "      <td>nl_36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>a</td>\n",
       "      <td>nl_33</td>\n",
       "      <td>nl_47</td>\n",
       "      <td>nl_42</td>\n",
       "      <td>nl_17</td>\n",
       "      <td>nl_37</td>\n",
       "      <td>nl_48</td>\n",
       "      <td>nl_24</td>\n",
       "      <td>nl_29</td>\n",
       "      <td>nl_44</td>\n",
       "      <td>nl_37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>a</td>\n",
       "      <td>nl_49</td>\n",
       "      <td>nl_28</td>\n",
       "      <td>nl_32</td>\n",
       "      <td>nl_40</td>\n",
       "      <td>nl_32</td>\n",
       "      <td>nl_26</td>\n",
       "      <td>nl_33</td>\n",
       "      <td>nl_4</td>\n",
       "      <td>nl_27</td>\n",
       "      <td>nl_41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>a</td>\n",
       "      <td>nl_21</td>\n",
       "      <td>nl_8</td>\n",
       "      <td>nl_46</td>\n",
       "      <td>nl_13</td>\n",
       "      <td>nl_30</td>\n",
       "      <td>nl_8</td>\n",
       "      <td>nl_28</td>\n",
       "      <td>nl_0</td>\n",
       "      <td>nl_1</td>\n",
       "      <td>nl_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   const_col noise_0 noise_1 noise_2 noise_3 noise_4 noise_5 noise_6 noise_7  \\\n",
       "0          a   nl_14   nl_48    nl_0   nl_36   nl_11   nl_37   nl_14    nl_0   \n",
       "1          a    nl_4   nl_45   nl_45    nl_6   nl_25   nl_10   nl_18   nl_37   \n",
       "2          a   nl_36   nl_38    nl_9   nl_34   nl_29   nl_49   nl_18   nl_14   \n",
       "3          a   nl_25   nl_31   nl_18   nl_36   nl_41   nl_30    nl_5   nl_31   \n",
       "4          a   nl_14   nl_24    nl_5    nl_5    nl_1   nl_23   nl_27   nl_42   \n",
       "..       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "95         a   nl_17   nl_13   nl_33   nl_13   nl_49   nl_31   nl_35   nl_32   \n",
       "96         a   nl_49   nl_43   nl_14    nl_3   nl_32   nl_47   nl_27   nl_23   \n",
       "97         a   nl_33   nl_47   nl_42   nl_17   nl_37   nl_48   nl_24   nl_29   \n",
       "98         a   nl_49   nl_28   nl_32   nl_40   nl_32   nl_26   nl_33    nl_4   \n",
       "99         a   nl_21    nl_8   nl_46   nl_13   nl_30    nl_8   nl_28    nl_0   \n",
       "\n",
       "   noise_8 noise_9  \n",
       "0    nl_28   nl_41  \n",
       "1    nl_19   nl_38  \n",
       "2     nl_0    nl_7  \n",
       "3    nl_21   nl_21  \n",
       "4    nl_34    nl_6  \n",
       "..     ...     ...  \n",
       "95   nl_17   nl_35  \n",
       "96   nl_30   nl_36  \n",
       "97   nl_44   nl_37  \n",
       "98   nl_27   nl_41  \n",
       "99    nl_1    nl_4  \n",
       "\n",
       "[100 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.35640831, -1.02966661,  0.10937507, -1.22461964,  1.98078615,\n",
       "       -0.08133883, -0.8407814 ,  0.17419664, -0.26695962,  1.81730992,\n",
       "       -0.40031323, -0.01591122,  0.06183108, -2.63659715, -1.23370528,\n",
       "       -1.5720175 , -0.30802748, -1.12459106,  0.80999499, -0.58387749,\n",
       "        0.56838667,  0.90359773,  0.98001598,  0.94958149, -0.15583399,\n",
       "        1.3929476 ,  0.4957971 , -0.18760574, -0.97232061, -0.1383119 ,\n",
       "       -1.98764929,  0.42246929, -0.25438059,  0.64496689, -0.12015076,\n",
       "       -0.48352493,  0.53825049,  1.23793055,  0.14021035,  1.38925737,\n",
       "        0.18708701,  0.45131922,  1.80806884, -0.51693141,  0.87514908,\n",
       "        0.36805093, -0.36548753, -1.56253055, -0.88706849,  0.57927198,\n",
       "       -0.2806769 , -0.07133204,  0.74667248, -0.81331984,  0.66688814,\n",
       "        1.03676875,  1.00533415,  0.83378592, -0.81403847, -2.26635425,\n",
       "       -0.99387029, -0.48577153, -0.51869578,  0.17533136, -0.79042072,\n",
       "       -0.88466057,  0.21123103,  1.68172973,  0.41984886, -2.41421325,\n",
       "        3.09279159,  0.35416444, -0.235704  , -1.06207208, -0.65270214,\n",
       "        1.27772083,  0.06024998,  0.76973631,  0.66210325,  0.45382781,\n",
       "        1.35590224,  0.84735606,  1.26620742,  1.24446983,  0.80523392,\n",
       "        1.08266673,  0.66324872,  0.61433843, -0.49462885,  1.62266163,\n",
       "        0.53028485,  0.48833726,  0.31907274, -0.04827782, -1.02430346,\n",
       "       -0.4880873 , -0.23963282, -1.74395751,  3.14710139, -2.43246331])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this data all \"noise variables\" are generated independently of the outcome or dependent variable `y_example`.  `const_col` is a variable that does not vary: it always has the value \"`a`\". We want to fit a linear regression model for `y_example` to the data. Of course, such a model should predict nothing, since there is no relationship between the inputs and the output.\n",
    "\n",
    "For our examples, we will re-encode each categorical variable into a single numerical encoding (here called a *target encoding*) where each level of the categorical variable is encoded as the (possibly smoothed) conditional mean of the *y* variable in the training set. Target encoding (and the similar [*impact coding*](https://www.r-bloggers.com/modeling-trick-impact-coding-of-categorical-variables-with-many-levels/)) can be useful when modeling with very high cardinality categorical variables (that is, categorical variables with a very large number of levels), or when modeling with many moderate cardinality categorical variables. In either case, target encoding, when used properly, is good for managing the \"variable blowup\" caused by encoding a single categorical variable into multiple indicator variables.\n",
    "\n",
    "For more discussion, see [this article](http://www.win-vector.com/blog/2012/07/modeling-trick-impact-coding-of-categorical-variables-with-many-levels/) and [this one](http://www.win-vector.com/blog/2019/11/when-cross-validation-is-more-powerful-than-regularization/). \n",
    "\n",
    "## The Case of No Cross-Method\n",
    "\n",
    "For this first example we will use [`category_encoders.target_encoder.TargetEncoder`](https://contrib.scikit-learn.org/categorical-encoding/targetencoder.html) to re-encode our categorical variables prior to a linear regression.  This target encoder re-encodes categorical variables as smoothed conditional estimates.\n",
    "\n",
    "On its own, the encoder is not cross-validated. This means high complexity explanatory variables hide their true number of degrees of freedom and leak information.  This causes the variables to over-fit on training data even when they are useless on test data, as we will demonstrate below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const_col</th>\n",
       "      <th>noise_0</th>\n",
       "      <th>noise_1</th>\n",
       "      <th>noise_2</th>\n",
       "      <th>noise_3</th>\n",
       "      <th>noise_4</th>\n",
       "      <th>noise_5</th>\n",
       "      <th>noise_6</th>\n",
       "      <th>noise_7</th>\n",
       "      <th>noise_8</th>\n",
       "      <th>noise_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.089719</td>\n",
       "      <td>0.878442</td>\n",
       "      <td>-0.279115</td>\n",
       "      <td>-0.134412</td>\n",
       "      <td>-0.293227</td>\n",
       "      <td>0.296913</td>\n",
       "      <td>-1.324166</td>\n",
       "      <td>0.448744</td>\n",
       "      <td>-0.547689</td>\n",
       "      <td>0.089719</td>\n",
       "      <td>1.090799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.089719</td>\n",
       "      <td>-0.864603</td>\n",
       "      <td>0.074604</td>\n",
       "      <td>0.372476</td>\n",
       "      <td>-0.707656</td>\n",
       "      <td>-0.025692</td>\n",
       "      <td>-0.310375</td>\n",
       "      <td>-0.312264</td>\n",
       "      <td>0.089719</td>\n",
       "      <td>-0.509022</td>\n",
       "      <td>0.035622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.089719</td>\n",
       "      <td>0.089719</td>\n",
       "      <td>0.122710</td>\n",
       "      <td>0.147765</td>\n",
       "      <td>-0.474352</td>\n",
       "      <td>1.145241</td>\n",
       "      <td>0.089719</td>\n",
       "      <td>-0.312264</td>\n",
       "      <td>0.491677</td>\n",
       "      <td>0.554715</td>\n",
       "      <td>0.127783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.089719</td>\n",
       "      <td>-0.179738</td>\n",
       "      <td>0.039331</td>\n",
       "      <td>-0.347868</td>\n",
       "      <td>-0.293227</td>\n",
       "      <td>-0.964601</td>\n",
       "      <td>-0.178340</td>\n",
       "      <td>0.034212</td>\n",
       "      <td>-0.236233</td>\n",
       "      <td>-0.637285</td>\n",
       "      <td>0.420169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.089719</td>\n",
       "      <td>0.878442</td>\n",
       "      <td>0.536216</td>\n",
       "      <td>-0.276830</td>\n",
       "      <td>0.770188</td>\n",
       "      <td>1.212039</td>\n",
       "      <td>0.640521</td>\n",
       "      <td>0.250698</td>\n",
       "      <td>0.384876</td>\n",
       "      <td>1.255121</td>\n",
       "      <td>-0.119833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.089719</td>\n",
       "      <td>0.073256</td>\n",
       "      <td>0.481162</td>\n",
       "      <td>-0.509693</td>\n",
       "      <td>-1.043418</td>\n",
       "      <td>0.129405</td>\n",
       "      <td>-0.038776</td>\n",
       "      <td>-0.180355</td>\n",
       "      <td>0.089719</td>\n",
       "      <td>0.092802</td>\n",
       "      <td>0.089719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.089719</td>\n",
       "      <td>1.086894</td>\n",
       "      <td>0.038942</td>\n",
       "      <td>-0.099632</td>\n",
       "      <td>0.053167</td>\n",
       "      <td>0.847704</td>\n",
       "      <td>0.089719</td>\n",
       "      <td>0.250698</td>\n",
       "      <td>0.232613</td>\n",
       "      <td>-0.200089</td>\n",
       "      <td>-0.125004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.089719</td>\n",
       "      <td>-0.113104</td>\n",
       "      <td>0.089719</td>\n",
       "      <td>0.089719</td>\n",
       "      <td>-0.128602</td>\n",
       "      <td>-1.213415</td>\n",
       "      <td>-0.569695</td>\n",
       "      <td>-0.185133</td>\n",
       "      <td>0.089719</td>\n",
       "      <td>0.089719</td>\n",
       "      <td>-0.264294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.089719</td>\n",
       "      <td>1.086894</td>\n",
       "      <td>0.885041</td>\n",
       "      <td>1.494380</td>\n",
       "      <td>0.182377</td>\n",
       "      <td>0.847704</td>\n",
       "      <td>0.933494</td>\n",
       "      <td>1.099114</td>\n",
       "      <td>1.340374</td>\n",
       "      <td>0.737929</td>\n",
       "      <td>1.090799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.089719</td>\n",
       "      <td>-1.162300</td>\n",
       "      <td>-1.210592</td>\n",
       "      <td>0.089719</td>\n",
       "      <td>-1.043418</td>\n",
       "      <td>0.089719</td>\n",
       "      <td>-0.288454</td>\n",
       "      <td>-1.162300</td>\n",
       "      <td>-0.547689</td>\n",
       "      <td>-0.466596</td>\n",
       "      <td>-0.963628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    const_col   noise_0   noise_1   noise_2   noise_3   noise_4   noise_5  \\\n",
       "0    0.089719  0.878442 -0.279115 -0.134412 -0.293227  0.296913 -1.324166   \n",
       "1    0.089719 -0.864603  0.074604  0.372476 -0.707656 -0.025692 -0.310375   \n",
       "2    0.089719  0.089719  0.122710  0.147765 -0.474352  1.145241  0.089719   \n",
       "3    0.089719 -0.179738  0.039331 -0.347868 -0.293227 -0.964601 -0.178340   \n",
       "4    0.089719  0.878442  0.536216 -0.276830  0.770188  1.212039  0.640521   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "95   0.089719  0.073256  0.481162 -0.509693 -1.043418  0.129405 -0.038776   \n",
       "96   0.089719  1.086894  0.038942 -0.099632  0.053167  0.847704  0.089719   \n",
       "97   0.089719 -0.113104  0.089719  0.089719 -0.128602 -1.213415 -0.569695   \n",
       "98   0.089719  1.086894  0.885041  1.494380  0.182377  0.847704  0.933494   \n",
       "99   0.089719 -1.162300 -1.210592  0.089719 -1.043418  0.089719 -0.288454   \n",
       "\n",
       "     noise_6   noise_7   noise_8   noise_9  \n",
       "0   0.448744 -0.547689  0.089719  1.090799  \n",
       "1  -0.312264  0.089719 -0.509022  0.035622  \n",
       "2  -0.312264  0.491677  0.554715  0.127783  \n",
       "3   0.034212 -0.236233 -0.637285  0.420169  \n",
       "4   0.250698  0.384876  1.255121 -0.119833  \n",
       "..       ...       ...       ...       ...  \n",
       "95 -0.180355  0.089719  0.092802  0.089719  \n",
       "96  0.250698  0.232613 -0.200089 -0.125004  \n",
       "97 -0.185133  0.089719  0.089719 -0.264294  \n",
       "98  1.099114  1.340374  0.737929  1.090799  \n",
       "99 -1.162300 -0.547689 -0.466596 -0.963628  \n",
       "\n",
       "[100 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te0 = category_encoders.target_encoder.TargetEncoder()\n",
    "d_coded_0 = te0. \\\n",
    "    fit_transform(d_example, y_example)\n",
    "d_coded_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.830</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.811</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   43.52</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 10 Mar 2020</td> <th>  Prob (F-statistic):</th> <td>5.29e-30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:01:32</td>     <th>  Log-Likelihood:    </th> <td> -60.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   142.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    89</td>      <th>  BIC:               </th> <td>   171.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const_col</th> <td>   -2.1941</td> <td>    0.558</td> <td>   -3.930</td> <td> 0.000</td> <td>   -3.303</td> <td>   -1.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>noise_0</th>   <td>    0.3103</td> <td>    0.116</td> <td>    2.665</td> <td> 0.009</td> <td>    0.079</td> <td>    0.542</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>noise_1</th>   <td>    0.2227</td> <td>    0.114</td> <td>    1.954</td> <td> 0.054</td> <td>   -0.004</td> <td>    0.449</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>noise_2</th>   <td>    0.6015</td> <td>    0.127</td> <td>    4.754</td> <td> 0.000</td> <td>    0.350</td> <td>    0.853</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>noise_3</th>   <td>    0.4393</td> <td>    0.126</td> <td>    3.499</td> <td> 0.001</td> <td>    0.190</td> <td>    0.689</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>noise_4</th>   <td>    0.4852</td> <td>    0.102</td> <td>    4.760</td> <td> 0.000</td> <td>    0.283</td> <td>    0.688</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>noise_5</th>   <td>    0.0554</td> <td>    0.121</td> <td>    0.459</td> <td> 0.647</td> <td>   -0.184</td> <td>    0.295</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>noise_6</th>   <td>    0.1714</td> <td>    0.115</td> <td>    1.490</td> <td> 0.140</td> <td>   -0.057</td> <td>    0.400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>noise_7</th>   <td>    0.2075</td> <td>    0.127</td> <td>    1.635</td> <td> 0.106</td> <td>   -0.045</td> <td>    0.460</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>noise_8</th>   <td>    0.4540</td> <td>    0.109</td> <td>    4.151</td> <td> 0.000</td> <td>    0.237</td> <td>    0.671</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>noise_9</th>   <td>    0.4056</td> <td>    0.107</td> <td>    3.792</td> <td> 0.000</td> <td>    0.193</td> <td>    0.618</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 5.411</td> <th>  Durbin-Watson:     </th> <td>   1.728</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.067</td> <th>  Jarque-Bera (JB):  </th> <td>   5.454</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.346</td> <th>  Prob(JB):          </th> <td>  0.0654</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.910</td> <th>  Cond. No.          </th> <td>    12.1</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.830\n",
       "Model:                            OLS   Adj. R-squared:                  0.811\n",
       "Method:                 Least Squares   F-statistic:                     43.52\n",
       "Date:                Tue, 10 Mar 2020   Prob (F-statistic):           5.29e-30\n",
       "Time:                        18:01:32   Log-Likelihood:                -60.214\n",
       "No. Observations:                 100   AIC:                             142.4\n",
       "Df Residuals:                      89   BIC:                             171.1\n",
       "Df Model:                          10                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const_col     -2.1941      0.558     -3.930      0.000      -3.303      -1.085\n",
       "noise_0        0.3103      0.116      2.665      0.009       0.079       0.542\n",
       "noise_1        0.2227      0.114      1.954      0.054      -0.004       0.449\n",
       "noise_2        0.6015      0.127      4.754      0.000       0.350       0.853\n",
       "noise_3        0.4393      0.126      3.499      0.001       0.190       0.689\n",
       "noise_4        0.4852      0.102      4.760      0.000       0.283       0.688\n",
       "noise_5        0.0554      0.121      0.459      0.647      -0.184       0.295\n",
       "noise_6        0.1714      0.115      1.490      0.140      -0.057       0.400\n",
       "noise_7        0.2075      0.127      1.635      0.106      -0.045       0.460\n",
       "noise_8        0.4540      0.109      4.151      0.000       0.237       0.671\n",
       "noise_9        0.4056      0.107      3.792      0.000       0.193       0.618\n",
       "==============================================================================\n",
       "Omnibus:                        5.411   Durbin-Watson:                   1.728\n",
       "Prob(Omnibus):                  0.067   Jarque-Bera (JB):                5.454\n",
       "Skew:                           0.346   Prob(JB):                       0.0654\n",
       "Kurtosis:                       3.910   Cond. No.                         12.1\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overfit_model = statsmodels.api.OLS(y_example, d_coded_0)\n",
    "overfit_result = overfit_model.fit()\n",
    "\n",
    "r2 = sklearn.metrics.r2_score(\n",
    "    y_true=y_example, \n",
    "    y_pred=overfit_result.predict(d_coded_0))\n",
    "assert r2 > 0.7\n",
    "r2\n",
    "\n",
    "overfit_result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the summary estimates a good adjusted R-squared (around 0.8), and finds many of the noise coefficients to be significant.  This is because the re-encoding step over-fit the data before we even got to the ordinary least squares regression.  We can confirm this by showing the model doesn't work on identically generated fresh data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.25111316272369755"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_test, y_test = mk_data(\n",
    "    nrow=100,\n",
    "    n_noise_var=10,\n",
    "    n_noise_level=50,\n",
    "    n_signal_var=0)\n",
    "\n",
    "d_test_coded = te0.transform(d_test)\n",
    "\n",
    "r2 = sklearn.metrics.r2_score(\n",
    "    y_true=y_test, \n",
    "    y_pred=overfit_result.predict(d_test_coded))\n",
    "assert r2 < 0.2\n",
    "\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be an insidious issue: over-estimating model performance, and also allowing complex noise variables to outcompete low-complexity but actually useful explanatory variables.\n",
    "\n",
    "Our advice to avoid this issue is to either use separate data for encoding and modeling, or use a cross-method when re-encoding the categorical variables.  This is what we are *very* careful to do correctly in [R vtreat](https://github.com/WinVector/vtreat) and [Python vtreat](https://github.com/WinVector/pyvtreat).\n",
    "\n",
    "## Leave-One-Out Cross-Methods Data Leak\n",
    "\n",
    "Let's take a quick look at how problems can arise even when using cross-methods.\n",
    "\n",
    "In our opinion, to minimize data leaks one should avoid using a deterministic cross method plan, which can often pass through undesirable incidental structure in the data.  As such, we advise against using a leave-one-out cross-plan in production.\n",
    "\n",
    "Leave-one-out leaks information in many places, including even in a constant column (a column that does not vary). To see this, let's try to fit a model for `y_example` using only `const_col`. First, we target-code `const_col`. We don't *need* to cross-validate a constant, but it is a problem that it doesn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const_col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.087026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.101026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.089521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.102996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.070618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.095556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.093046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.108241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.058837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.115196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    const_col\n",
       "0    0.087026\n",
       "1    0.101026\n",
       "2    0.089521\n",
       "3    0.102996\n",
       "4    0.070618\n",
       "..        ...\n",
       "95   0.095556\n",
       "96   0.093046\n",
       "97   0.108241\n",
       "98   0.058837\n",
       "99   0.115196\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_one_out = sklearn.model_selection.LeaveOneOut()\n",
    "\n",
    "# TransformAdapter adapts the TargetEncoder object for cross-methods\n",
    "te2 = TransformerAdapter(\n",
    "    category_encoders.target_encoder.TargetEncoder())\n",
    "\n",
    "# Build the cross-validated encoding of the training data\n",
    "# For use in training the model\n",
    "cross_frame_0 = sklearn.model_selection.cross_val_predict(\n",
    "    te2, \n",
    "    d_example[['const_col']], # just look at the constant column\n",
    "    y_example, \n",
    "    cv=cv_one_out)\n",
    "cross_frame_0 = pandas.DataFrame(cross_frame_0)\n",
    "cross_frame_0.columns = ['const_col']\n",
    "\n",
    "# This is the \"transformed\" training data\n",
    "cross_frame_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the re-encoding of the constant column varies per row. This is because in cross-methods the prediction is a function of the input *plus* the fold-id; it is *not* a function of the input alone.  For leave-one-out encoding of a constant, the encoding is `code[i] = (m * mean(y) - y[i])/(m - 1)`: the grand mean with the `i`-th row held out.  But this is equal to `m * mean(y) / (m - 1) - y[i] / (m - 1)`.  This means during training it is trivial to read off the `y`-values from the re-encoded constant column. To see this, we fit a linear regression model for `y_example` as a function of `cross_frame_0`, the re-encoded training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   1.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   1.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.305e+31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 10 Mar 2020</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:01:34</td>     <th>  Log-Likelihood:    </th> <td>  3204.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>  -6404.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>  -6399.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    8.9719</td> <td> 2.48e-15</td> <td> 3.62e+15</td> <td> 0.000</td> <td>    8.972</td> <td>    8.972</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>  -99.0000</td> <td> 2.74e-14</td> <td>-3.61e+15</td> <td> 0.000</td> <td>  -99.000</td> <td>  -99.000</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.122</td> <th>  Durbin-Watson:     </th> <td>   0.324</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.941</td> <th>  Jarque-Bera (JB):  </th> <td>   0.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.066</td> <th>  Prob(JB):          </th> <td>   0.876</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.784</td> <th>  Cond. No.          </th> <td>    93.1</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       1.000\n",
       "Model:                            OLS   Adj. R-squared:                  1.000\n",
       "Method:                 Least Squares   F-statistic:                 1.305e+31\n",
       "Date:                Tue, 10 Mar 2020   Prob (F-statistic):               0.00\n",
       "Time:                        18:01:34   Log-Likelihood:                 3204.2\n",
       "No. Observations:                 100   AIC:                            -6404.\n",
       "Df Residuals:                      98   BIC:                            -6399.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          8.9719   2.48e-15   3.62e+15      0.000       8.972       8.972\n",
       "x1           -99.0000   2.74e-14  -3.61e+15      0.000     -99.000     -99.000\n",
       "==============================================================================\n",
       "Omnibus:                        0.122   Durbin-Watson:                   0.324\n",
       "Prob(Omnibus):                  0.941   Jarque-Bera (JB):                0.266\n",
       "Skew:                          -0.066   Prob(JB):                        0.876\n",
       "Kurtosis:                       2.784   Cond. No.                         93.1\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit a linear regression model as a function of the \n",
    "# target encoded training data\n",
    "overfit_model_2 = statsmodels.api.OLS(\n",
    "    y_example, \n",
    "    statsmodels.api.add_constant(   # add the DC intercept term\n",
    "        cross_frame_0.values, \n",
    "        has_constant='add'))\n",
    "overfit_result_2 = overfit_model_2.fit()\n",
    "\n",
    "# calculate R-squared\n",
    "r2 = sklearn.metrics.r2_score(\n",
    "    y_true=y_example, \n",
    "    y_pred=overfit_result_2.predict(\n",
    "        statsmodels.api.add_constant(\n",
    "            cross_frame_0.values,\n",
    "            has_constant='add')))\n",
    "assert r2 > 0.9\n",
    "\n",
    "overfit_result_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see here is that we have apparently fit a perfect linear regression model for `y_example` using only a (re-encoded) constant input!\n",
    "\n",
    "A crucial point to notice is that the regression used large magnitude (and negative) coefficients.  This is because there is a data leak, but it is low magnitude.  So to exploit the data leak we have to scale it up quite a bit. This is typical: many results in the literature that show the efficacy of cross-methods do so by proving that with high probability that method is very near a correct result *in norm*: that is, the norm of the difference between the cross-validated result and the true result is small. Our encoding was close to the true result, in this sense, but still represented a data leak, one that linear regression was able to exploit.\n",
    "\n",
    "Again, this chimeric \"well-fit model\" will be useless on new data. Let's try it.\n",
    "\n",
    "To try to work with new data, we want the target encoder to be fit on all the original training data. Note that the [`vtreat`](https://github.com/WinVector/pyvtreat) package, which we will discuss later, does not require this extra step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerAdapter(model=TargetEncoder(cols=['const_col'], drop_invariant=False,\n",
       "                                       handle_missing='value',\n",
       "                                       handle_unknown='value',\n",
       "                                       min_samples_leaf=1, return_df=True,\n",
       "                                       smoothing=1.0, verbose=0))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refit the target encoder on original data for later use\n",
    "te2.fit(d_example[['const_col']], y_example) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the encoder to  new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.012108850282850137"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now target encode the new test data\n",
    "d_test_coded_2 = te2.transform(d_test[['const_col']])\n",
    "\n",
    "overfit_test_pred_2 = overfit_result_2.predict(\n",
    "    statsmodels.api.add_constant(\n",
    "        d_test_coded_2.values, \n",
    "        has_constant='add'))\n",
    "\n",
    "r2 = sklearn.metrics.r2_score(y_true=y_test, y_pred=overfit_test_pred_2)\n",
    "assert r2 < 0.2\n",
    "\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leak we demonstrated above is one of the reasons `vtreat` uses impact codes (conditional difference from the mean) instead of target codes (conditional means). With cross-validated impact coding, constant variables will *always* code to zero, effectively identifying them as uninformative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Method Done Correctly \n",
    "\n",
    "Now lets look at shuffled (pseudo-random) `k`-way cross method version of the target encoding.\n",
    "\n",
    "For this example we look at all the input columns, and we will use 3-fold cross method. For a 3-fold cross-plan the bias from any one column is small, but many columns together will leak information.  We will demonstrate this next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const_col</th>\n",
       "      <th>noise_0</th>\n",
       "      <th>noise_1</th>\n",
       "      <th>noise_2</th>\n",
       "      <th>noise_3</th>\n",
       "      <th>noise_4</th>\n",
       "      <th>noise_5</th>\n",
       "      <th>noise_6</th>\n",
       "      <th>noise_7</th>\n",
       "      <th>noise_8</th>\n",
       "      <th>noise_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>-0.572658</td>\n",
       "      <td>0.034988</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.037668</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.053041</td>\n",
       "      <td>-0.762554</td>\n",
       "      <td>0.687571</td>\n",
       "      <td>0.053041</td>\n",
       "      <td>0.053041</td>\n",
       "      <td>0.053041</td>\n",
       "      <td>0.054035</td>\n",
       "      <td>0.053041</td>\n",
       "      <td>0.053041</td>\n",
       "      <td>-0.711652</td>\n",
       "      <td>0.053041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.573658</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>-0.055440</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>-0.400298</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.183377</td>\n",
       "      <td>-0.246219</td>\n",
       "      <td>0.468289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.040946</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.814798</td>\n",
       "      <td>0.231655</td>\n",
       "      <td>-0.258593</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>-0.843376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.369370</td>\n",
       "      <td>0.109383</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>-0.040279</td>\n",
       "      <td>0.160505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.053041</td>\n",
       "      <td>0.053041</td>\n",
       "      <td>0.053041</td>\n",
       "      <td>0.215618</td>\n",
       "      <td>0.053041</td>\n",
       "      <td>1.096685</td>\n",
       "      <td>0.053041</td>\n",
       "      <td>0.669725</td>\n",
       "      <td>0.053041</td>\n",
       "      <td>-0.377680</td>\n",
       "      <td>-0.067086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.192124</td>\n",
       "      <td>-0.843376</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.100504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>-0.355399</td>\n",
       "      <td>0.098604</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.003025</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.055096</td>\n",
       "      <td>-0.309072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    const_col   noise_0   noise_1   noise_2   noise_3   noise_4   noise_5  \\\n",
       "0    0.055096  0.055096 -0.572658  0.034988  0.055096  0.037668  0.055096   \n",
       "1    0.053041 -0.762554  0.687571  0.053041  0.053041  0.053041  0.054035   \n",
       "2    0.055096  0.055096  0.055096  0.055096  0.055096  0.055096  0.055096   \n",
       "3    0.160505  0.160505  0.160505 -0.055440  0.160505 -0.400298  0.160505   \n",
       "4    0.160505  0.160505  0.040946  0.160505  0.160505  0.814798  0.231655   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "95   0.160505  0.160505  0.160505  0.160505  0.160505  0.369370  0.109383   \n",
       "96   0.053041  0.053041  0.053041  0.215618  0.053041  1.096685  0.053041   \n",
       "97   0.160505  0.160505  0.160505  0.160505  0.192124 -0.843376  0.160505   \n",
       "98   0.055096  0.055096  0.055096  0.055096 -0.355399  0.098604  0.055096   \n",
       "99   0.055096  0.055096  0.003025  0.055096  0.055096  0.055096  0.055096   \n",
       "\n",
       "     noise_6   noise_7   noise_8   noise_9  \n",
       "0   0.055096  0.055096  0.055096  0.055096  \n",
       "1   0.053041  0.053041 -0.711652  0.053041  \n",
       "2   0.055096  0.573658  0.055096  0.055096  \n",
       "3   0.160505  0.183377 -0.246219  0.468289  \n",
       "4  -0.258593  0.160505  0.160505 -0.843376  \n",
       "..       ...       ...       ...       ...  \n",
       "95  0.160505  0.160505 -0.040279  0.160505  \n",
       "96  0.669725  0.053041 -0.377680 -0.067086  \n",
       "97  0.160505  0.160505  0.160505  0.100504  \n",
       "98  0.055096  0.055096  0.055096  0.055096  \n",
       "99  0.055096  0.055096  0.055096 -0.309072  \n",
       "\n",
       "[100 rows x 11 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a shuffled cross-plan\n",
    "# http://www.win-vector.com/blog/2020/03/python-data-science-tip-dont-use-default-cross-validation-settings/\n",
    "cvstrat = sklearn.model_selection.KFold(\n",
    "    shuffle=True, \n",
    "    n_splits=3,\n",
    "    random_state=prng)\n",
    "\n",
    "te = category_encoders.target_encoder.TargetEncoder()\n",
    "cross_frame = sklearn.model_selection.cross_val_predict(\n",
    "    TransformerAdapter(te), \n",
    "    d_example, \n",
    "    y_example, \n",
    "    cv=cvstrat)\n",
    "\n",
    "# Build the transformed training data\n",
    "cross_frame = pandas.DataFrame(cross_frame)\n",
    "cross_frame.rename(\n",
    "    columns={i: d_example.columns[i] for i in\n",
    "             range(d_example.shape[1])}, inplace=True)\n",
    "\n",
    "cross_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the encoded columns vary as a function of both the cross-fold and the input, so even the constant column varies after the re-encoding.\n",
    "\n",
    "However, due to the proper use of a randomized cross-method these columns will still appear to be useless noise.  Their correlation with the output was not substantially elevated during the encoding. Conversely useful columns, if there were any, would remain useful in the re-encoding.  Below, we show that the re-encoded noise variables remain uncorrelated with the explanatory variable when we fit a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.137</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1.265</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 10 Mar 2020</td> <th>  Prob (F-statistic):</th>  <td> 0.258</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:01:35</td>     <th>  Log-Likelihood:    </th> <td> -141.53</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   307.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    88</td>      <th>  BIC:               </th> <td>   338.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.1989</td> <td>    0.231</td> <td>    0.861</td> <td> 0.392</td> <td>   -0.260</td> <td>    0.658</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -0.4560</td> <td>    2.452</td> <td>   -0.186</td> <td> 0.853</td> <td>   -5.328</td> <td>    4.416</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -0.0941</td> <td>    0.389</td> <td>   -0.242</td> <td> 0.809</td> <td>   -0.867</td> <td>    0.679</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.1700</td> <td>    0.290</td> <td>   -0.587</td> <td> 0.559</td> <td>   -0.746</td> <td>    0.406</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -0.4451</td> <td>    0.445</td> <td>   -1.001</td> <td> 0.320</td> <td>   -1.329</td> <td>    0.439</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   -0.6703</td> <td>    0.382</td> <td>   -1.753</td> <td> 0.083</td> <td>   -1.430</td> <td>    0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    0.4572</td> <td>    0.273</td> <td>    1.674</td> <td> 0.098</td> <td>   -0.085</td> <td>    1.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>   -0.3084</td> <td>    0.405</td> <td>   -0.761</td> <td> 0.449</td> <td>   -1.114</td> <td>    0.497</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>    0.0349</td> <td>    0.275</td> <td>    0.127</td> <td> 0.899</td> <td>   -0.512</td> <td>    0.581</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>   -0.5848</td> <td>    0.431</td> <td>   -1.357</td> <td> 0.178</td> <td>   -1.441</td> <td>    0.272</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>    0.4283</td> <td>    0.294</td> <td>    1.457</td> <td> 0.149</td> <td>   -0.156</td> <td>    1.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>    0.3355</td> <td>    0.255</td> <td>    1.316</td> <td> 0.192</td> <td>   -0.171</td> <td>    0.842</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.851</td> <th>  Durbin-Watson:     </th> <td>   1.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.653</td> <th>  Jarque-Bera (JB):  </th> <td>   0.395</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.045</td> <th>  Prob(JB):          </th> <td>   0.821</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.294</td> <th>  Cond. No.          </th> <td>    23.9</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.137\n",
       "Model:                            OLS   Adj. R-squared:                  0.029\n",
       "Method:                 Least Squares   F-statistic:                     1.265\n",
       "Date:                Tue, 10 Mar 2020   Prob (F-statistic):              0.258\n",
       "Time:                        18:01:35   Log-Likelihood:                -141.53\n",
       "No. Observations:                 100   AIC:                             307.1\n",
       "Df Residuals:                      88   BIC:                             338.3\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.1989      0.231      0.861      0.392      -0.260       0.658\n",
       "x1            -0.4560      2.452     -0.186      0.853      -5.328       4.416\n",
       "x2            -0.0941      0.389     -0.242      0.809      -0.867       0.679\n",
       "x3            -0.1700      0.290     -0.587      0.559      -0.746       0.406\n",
       "x4            -0.4451      0.445     -1.001      0.320      -1.329       0.439\n",
       "x5            -0.6703      0.382     -1.753      0.083      -1.430       0.090\n",
       "x6             0.4572      0.273      1.674      0.098      -0.085       1.000\n",
       "x7            -0.3084      0.405     -0.761      0.449      -1.114       0.497\n",
       "x8             0.0349      0.275      0.127      0.899      -0.512       0.581\n",
       "x9            -0.5848      0.431     -1.357      0.178      -1.441       0.272\n",
       "x10            0.4283      0.294      1.457      0.149      -0.156       1.012\n",
       "x11            0.3355      0.255      1.316      0.192      -0.171       0.842\n",
       "==============================================================================\n",
       "Omnibus:                        0.851   Durbin-Watson:                   1.968\n",
       "Prob(Omnibus):                  0.653   Jarque-Bera (JB):                0.395\n",
       "Skew:                           0.045   Prob(JB):                        0.821\n",
       "Kurtosis:                       3.294   Cond. No.                         23.9\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proper_fit_model = statsmodels.api.OLS(\n",
    "    y_example, \n",
    "    statsmodels.api.add_constant(\n",
    "        cross_frame.values, \n",
    "        has_constant='add'))\n",
    "proper_fit_result = proper_fit_model.fit()\n",
    "\n",
    "r2 = sklearn.metrics.r2_score(\n",
    "    y_true=y_example, \n",
    "    y_pred=proper_fit_result.predict(\n",
    "        statsmodels.api.add_constant(\n",
    "            cross_frame.values,\n",
    "            has_constant='add')))\n",
    "assert r2 < 0.2\n",
    "\n",
    "proper_fit_result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm from the summary that:\n",
    "\n",
    "  * The overall model does not claim to be predictive\n",
    "  * The individual coefficients appear to have non-significant p-values\n",
    "\n",
    "This re-encoding failed to mess up the linear regression: uninformative variables correctly appear uninformative.  This appears to be good as we could hope for in this situation.\n",
    "\n",
    "## Showing The Leak is Still There\n",
    "\n",
    "However, there is still a data leak!\n",
    "\n",
    "Code designed to look for the leak can find it. First we can identify from the encoded variables where the cross-folds are: wherever two rows have the same value for a variable, but see different encodings.  We demonstrate recovering the cross-plan here, by building a data leak machine that can recover `y_example` from the re-encoded noise variables, which should not be possible for variables unrelated to the output.\n",
    "\n",
    "Here, we solve for complement sets: in this case, the complements of the folds of the cross-plan. Specifically, if row `i` is in fold `a`, the complement set for row `i` is the union of folds `b` and `c` (assuming a 3-fold cross-plan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>complement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[1, 3, 4, 5, 7, 10, 12, 14, 15, 16, 17, 19, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 2, 3, 4, 5, 6, 7, 8, 9, 11, 13, 14, 15, 17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[1, 3, 4, 5, 7, 10, 12, 14, 15, 16, 17, 19, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1, 2, 6, 8, 9, 10, 11, 12, 13, 16, 18, 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[0, 1, 2, 6, 8, 9, 10, 11, 12, 13, 16, 18, 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>[0, 1, 2, 6, 8, 9, 10, 11, 12, 13, 16, 18, 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>[0, 2, 3, 4, 5, 6, 7, 8, 9, 11, 13, 14, 15, 17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>[0, 1, 2, 6, 8, 9, 10, 11, 12, 13, 16, 18, 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>[1, 3, 4, 5, 7, 10, 12, 14, 15, 16, 17, 19, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>[1, 3, 4, 5, 7, 10, 12, 14, 15, 16, 17, 19, 20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    idx                                         complement\n",
       "0     0  [1, 3, 4, 5, 7, 10, 12, 14, 15, 16, 17, 19, 20...\n",
       "1     1  [0, 2, 3, 4, 5, 6, 7, 8, 9, 11, 13, 14, 15, 17...\n",
       "2     2  [1, 3, 4, 5, 7, 10, 12, 14, 15, 16, 17, 19, 20...\n",
       "3     3  [0, 1, 2, 6, 8, 9, 10, 11, 12, 13, 16, 18, 20,...\n",
       "4     4  [0, 1, 2, 6, 8, 9, 10, 11, 12, 13, 16, 18, 20,...\n",
       "..  ...                                                ...\n",
       "95   95  [0, 1, 2, 6, 8, 9, 10, 11, 12, 13, 16, 18, 20,...\n",
       "96   96  [0, 2, 3, 4, 5, 6, 7, 8, 9, 11, 13, 14, 15, 17...\n",
       "97   97  [0, 1, 2, 6, 8, 9, 10, 11, 12, 13, 16, 18, 20,...\n",
       "98   98  [1, 3, 4, 5, 7, 10, 12, 14, 15, 16, 17, 19, 20...\n",
       "99   99  [1, 3, 4, 5, 7, 10, 12, 14, 15, 16, 17, 19, 20...\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_solution = solve_for_partition(d_example, cross_frame)\n",
    "\n",
    "partition_solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the target encoder computes a conditional smoothed average of the dependent variable. We can use this knowledge to collect relations of the form `cross_frame[i, j] = dot(wts[i, j], y_example) + b`. \n",
    "\n",
    "`cross_frame[i, j]` is known. `wts[i, j]` is gotten by reading the [source code for the target encoder](https://github.com/scikit-learn-contrib/categorical-encoding/blob/master/category_encoders/target_encoder.py). We will solve for `y_example` and `b`.\n",
    " \n",
    "We are proving there is a data leak by recovering `y_example` from the original input variable frame `d_example`, and the re-coded data frame `cross_frame`. There are ways to demonstrate the data leak without using `d_example`, but using `d_example` is simpler ot show.\n",
    "\n",
    "We start with a new function `target_encoder_weight_rule` that is our adaptation of the original target encoder source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def target_encoder_weight_rule(\n",
    "    *, nrow, partition_indexes, value_indexes,\n",
    "        min_samples_leaf=1, smoothing=1.0):\n",
    "    if (partition_indexes is None) or (len(partition_indexes) < 1):\n",
    "        return None\n",
    "    res = numpy.zeros(nrow)\n",
    "    prior_w = 1 / len(partition_indexes)\n",
    "    if (value_indexes is None) or (len(value_indexes) <= 1):\n",
    "        res[partition_indexes] = prior_w\n",
    "        return res\n",
    "    stats_count = len(value_indexes)\n",
    "    stats_mean_w = 1 / stats_count\n",
    "    smoove = 1 / (1 + numpy.exp(-(stats_count - min_samples_leaf) / smoothing))\n",
    "    res[partition_indexes] = prior_w * (1 - smoove)\n",
    "    res[value_indexes] = res[value_indexes] + stats_mean_w * smoove\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the function to capture the block of weights (`relns_x`) and the map from the original training data elements to their cross-frame encodings (`relns_y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.004014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.369543</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.369543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.004014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1100 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0     0.000000  0.015152  0.000000  0.015152  0.015152  0.015152  0.000000   \n",
       "1     0.014925  0.000000  0.014925  0.014925  0.014925  0.014925  0.014925   \n",
       "2     0.000000  0.015152  0.000000  0.015152  0.015152  0.015152  0.000000   \n",
       "3     0.014925  0.014925  0.014925  0.000000  0.000000  0.000000  0.014925   \n",
       "4     0.014925  0.014925  0.014925  0.000000  0.000000  0.000000  0.014925   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1095  0.004014  0.004014  0.004014  0.000000  0.000000  0.000000  0.004014   \n",
       "1096  0.000000  0.015152  0.000000  0.015152  0.015152  0.015152  0.000000   \n",
       "1097  0.000000  0.015152  0.000000  0.015152  0.015152  0.015152  0.000000   \n",
       "1098  0.004014  0.000000  0.004014  0.004014  0.004014  0.004014  0.369543   \n",
       "1099  0.000000  0.015152  0.000000  0.015152  0.015152  0.015152  0.000000   \n",
       "\n",
       "            7         8         9   ...        90        91        92  \\\n",
       "0     0.015152  0.000000  0.000000  ...  0.015152  0.000000  0.015152   \n",
       "1     0.014925  0.014925  0.014925  ...  0.000000  0.014925  0.000000   \n",
       "2     0.015152  0.000000  0.000000  ...  0.015152  0.000000  0.015152   \n",
       "3     0.000000  0.014925  0.014925  ...  0.014925  0.014925  0.014925   \n",
       "4     0.000000  0.014925  0.014925  ...  0.014925  0.014925  0.014925   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1095  0.000000  0.004014  0.004014  ...  0.004014  0.004014  0.004014   \n",
       "1096  0.015152  0.000000  0.000000  ...  0.015152  0.000000  0.015152   \n",
       "1097  0.015152  0.000000  0.000000  ...  0.015152  0.000000  0.015152   \n",
       "1098  0.004014  0.004014  0.004014  ...  0.000000  0.369543  0.000000   \n",
       "1099  0.015152  0.000000  0.000000  ...  0.015152  0.000000  0.015152   \n",
       "\n",
       "            93        94        95        96        97        98        99  \n",
       "0     0.015152  0.015152  0.015152  0.015152  0.015152  0.000000  0.000000  \n",
       "1     0.000000  0.014925  0.014925  0.000000  0.014925  0.014925  0.014925  \n",
       "2     0.015152  0.015152  0.015152  0.015152  0.015152  0.000000  0.000000  \n",
       "3     0.014925  0.000000  0.000000  0.014925  0.000000  0.014925  0.014925  \n",
       "4     0.014925  0.000000  0.000000  0.014925  0.000000  0.014925  0.014925  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1095  0.004014  0.000000  0.000000  0.004014  0.000000  0.004014  0.004014  \n",
       "1096  0.015152  0.015152  0.015152  0.015152  0.015152  0.000000  0.000000  \n",
       "1097  0.015152  0.015152  0.015152  0.015152  0.015152  0.000000  0.000000  \n",
       "1098  0.000000  0.004014  0.004014  0.000000  0.004014  0.004014  0.004014  \n",
       "1099  0.015152  0.015152  0.015152  0.015152  0.015152  0.000000  0.000000  \n",
       "\n",
       "[1100 rows x 100 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relns_x, relns_y = collect_relations(\n",
    "    d_original=d_example,\n",
    "    d_coded=cross_frame,\n",
    "    d_partition=partition_solution,\n",
    "    est_fn=target_encoder_weight_rule)\n",
    "\n",
    "relns_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.055096</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.053041</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.055096</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.160505</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.160505</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>0.267566</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>nl_31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>0.055096</td>\n",
       "      <td>81</td>\n",
       "      <td>10</td>\n",
       "      <td>nl_46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>0.055096</td>\n",
       "      <td>91</td>\n",
       "      <td>10</td>\n",
       "      <td>nl_18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>-0.114564</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>nl_18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>0.055096</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>nl_18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          code   i   j  level\n",
       "0     0.055096   0   0      a\n",
       "1     0.053041   1   0      a\n",
       "2     0.055096   2   0      a\n",
       "3     0.160505   3   0      a\n",
       "4     0.160505   4   0      a\n",
       "...        ...  ..  ..    ...\n",
       "1095  0.267566  15  10  nl_31\n",
       "1096  0.055096  81  10  nl_46\n",
       "1097  0.055096  91  10  nl_18\n",
       "1098 -0.114564  43  10  nl_18\n",
       "1099  0.055096   6  10  nl_18\n",
       "\n",
       "[1100 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relns_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, `relns_y$code[k]` is `cross_frame[relns_y$i[k], relns_y$j[k]]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We picked up around 1000 linear relations between the 100 `y_example` values (which we are treating as unknowns) and individual entries from the encoded data frame (which we are saving in `relns_y`).  Many of these are going to be redundant, but we have enough of them to solve for `y_example`:\n",
    "\n",
    "Let's call the matrix `[1 relns_x]` \"*A*\" (denoting that we added an initial column of ones), and the column vector `[b y_example]`\"*y*\". Then we expect that in matrix terms, *A* *y* = `relns_y$code`. This means *y* is the solution to linear equations, or a linear regression in our known quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.38286542, -1.00067254,  0.13751294, -1.19657209,  2.00542574,\n",
       "       -0.05263463, -0.81119794,  0.20031859, -0.2395179 ,  1.8427718 ,\n",
       "       -0.37208945,  0.01201268,  0.09141795, -2.6040474 , -1.20241303,\n",
       "       -1.54167653, -0.27971238, -1.09557047,  0.83786987, -0.55641342,\n",
       "        0.59636352,  0.92991006,  1.00825944,  0.97554531, -0.12696112,\n",
       "        1.41833807,  0.52306021, -0.15926224, -0.94035348, -0.1096274 ,\n",
       "       -1.95732204,  0.44977575, -0.22599382,  0.67050963, -0.09243166,\n",
       "       -0.45687631,  0.5651105 ,  1.26497826,  0.16751519,  1.41478549,\n",
       "        0.21518778,  0.47819745,  1.83465353, -0.48868329,  0.90191182,\n",
       "        0.39574586, -0.33710389, -1.5322998 , -0.85735641,  0.60634676,\n",
       "       -0.2537905 , -0.0442353 ,  0.77219068, -0.7843696 ,  0.69461054,\n",
       "        1.06297566,  1.03153519,  0.86000347, -0.78408083, -2.23598836,\n",
       "       -0.96249331, -0.45846078, -0.49024196,  0.20390794, -0.76087746,\n",
       "       -0.85466415,  0.23548188,  1.70830328,  0.44751128, -2.38399337,\n",
       "        3.11638852,  0.38140508, -0.2077327 , -1.03261288, -0.62524334,\n",
       "        1.30351635,  0.08753775,  0.79674417,  0.68742663,  0.4801845 ,\n",
       "        1.38062493,  0.87712229,  1.29332496,  1.26959284,  0.8321719 ,\n",
       "        1.10976459,  0.69073894,  0.64109561, -0.46635664,  1.65171781,\n",
       "        0.55929975,  0.51407236,  0.34689748, -0.0200642 , -0.99622881,\n",
       "       -0.45757837, -0.21019318, -1.7123978 ,  3.17090358, -2.40283436])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recover_model = sklearn.linear_model.Ridge(\n",
    "    alpha = 1.0e-3, \n",
    "    normalize=True)\n",
    "recover_model.fit(relns_x, relns_y['code'])\n",
    "y_ests = recover_model.coef_\n",
    "\n",
    "y_ests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these recovered estimates are in fact the original values of `y_example`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993249813376227"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2 = sklearn.metrics.r2_score(y_true=y_example, y_pred=y_ests)\n",
    "assert r2 > 0.9\n",
    "\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_frame = pandas.DataFrame({\n",
    "    'y_example': y_example,\n",
    "    'y_estimated': y_ests\n",
    "})\n",
    "sbn = seaborn.scatterplot(\n",
    "    'y_estimated', 'y_example', data=plt_frame)\n",
    "sbn.set_title(\n",
    "    \"actual ys as a function of estimates from pure noise variables\")\n",
    "true_range = [numpy.min(plt_frame['y_example']), \n",
    "              numpy.max(plt_frame['y_example'])]\n",
    "_ = matplotlib.pyplot.plot(\n",
    "        true_range, true_range,\n",
    "        color='orange', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again: what we have shown is that even when we target encode a set of pure noise variables using cross-methods, we can still recover `y_example` from the encoded variables. This means that the encoded noise variables can still potentially appear informative about `y_example` when used in a downstream model: cross-methods still leak information about the output variable.\n",
    "\n",
    "Does this mean we shouldn't use cross-methods? Not necessarily. We'll discuss this more, below.\n",
    "\n",
    "## Correct Cross Methods, The General Case\n",
    "\n",
    "Before we get to the question of whether we should use cross-methods at all, let's look at the general case, where the data has both noise variables and signal carrying variables. For this example, we'll use 100 rows of ten noise variables and five signal-carrying variables, all of fifty levels each. We'll throw a constant column in there, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d_example_s, y_example_s = mk_data(\n",
    "    nrow=100,\n",
    "    n_noise_var=10,\n",
    "    n_noise_level=50,\n",
    "    n_signal_var=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const_col</th>\n",
       "      <th>noise_0</th>\n",
       "      <th>noise_1</th>\n",
       "      <th>noise_2</th>\n",
       "      <th>noise_3</th>\n",
       "      <th>noise_4</th>\n",
       "      <th>noise_5</th>\n",
       "      <th>noise_6</th>\n",
       "      <th>noise_7</th>\n",
       "      <th>noise_8</th>\n",
       "      <th>noise_9</th>\n",
       "      <th>signal_0</th>\n",
       "      <th>signal_1</th>\n",
       "      <th>signal_2</th>\n",
       "      <th>signal_3</th>\n",
       "      <th>signal_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>nl_46</td>\n",
       "      <td>nl_41</td>\n",
       "      <td>nl_21</td>\n",
       "      <td>nl_49</td>\n",
       "      <td>nl_16</td>\n",
       "      <td>nl_17</td>\n",
       "      <td>nl_42</td>\n",
       "      <td>nl_16</td>\n",
       "      <td>nl_34</td>\n",
       "      <td>nl_9</td>\n",
       "      <td>a</td>\n",
       "      <td>b</td>\n",
       "      <td>a</td>\n",
       "      <td>b</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>nl_16</td>\n",
       "      <td>nl_27</td>\n",
       "      <td>nl_0</td>\n",
       "      <td>nl_5</td>\n",
       "      <td>nl_41</td>\n",
       "      <td>nl_5</td>\n",
       "      <td>nl_40</td>\n",
       "      <td>nl_23</td>\n",
       "      <td>nl_27</td>\n",
       "      <td>nl_42</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>nl_9</td>\n",
       "      <td>nl_34</td>\n",
       "      <td>nl_8</td>\n",
       "      <td>nl_35</td>\n",
       "      <td>nl_0</td>\n",
       "      <td>nl_24</td>\n",
       "      <td>nl_46</td>\n",
       "      <td>nl_35</td>\n",
       "      <td>nl_28</td>\n",
       "      <td>nl_25</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>b</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>nl_40</td>\n",
       "      <td>nl_31</td>\n",
       "      <td>nl_25</td>\n",
       "      <td>nl_35</td>\n",
       "      <td>nl_37</td>\n",
       "      <td>nl_19</td>\n",
       "      <td>nl_20</td>\n",
       "      <td>nl_46</td>\n",
       "      <td>nl_38</td>\n",
       "      <td>nl_9</td>\n",
       "      <td>b</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>nl_16</td>\n",
       "      <td>nl_35</td>\n",
       "      <td>nl_45</td>\n",
       "      <td>nl_28</td>\n",
       "      <td>nl_27</td>\n",
       "      <td>nl_44</td>\n",
       "      <td>nl_1</td>\n",
       "      <td>nl_12</td>\n",
       "      <td>nl_18</td>\n",
       "      <td>nl_31</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>a</td>\n",
       "      <td>nl_24</td>\n",
       "      <td>nl_2</td>\n",
       "      <td>nl_4</td>\n",
       "      <td>nl_12</td>\n",
       "      <td>nl_44</td>\n",
       "      <td>nl_8</td>\n",
       "      <td>nl_0</td>\n",
       "      <td>nl_7</td>\n",
       "      <td>nl_48</td>\n",
       "      <td>nl_47</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>a</td>\n",
       "      <td>nl_9</td>\n",
       "      <td>nl_10</td>\n",
       "      <td>nl_17</td>\n",
       "      <td>nl_39</td>\n",
       "      <td>nl_2</td>\n",
       "      <td>nl_31</td>\n",
       "      <td>nl_4</td>\n",
       "      <td>nl_24</td>\n",
       "      <td>nl_12</td>\n",
       "      <td>nl_6</td>\n",
       "      <td>b</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>a</td>\n",
       "      <td>nl_9</td>\n",
       "      <td>nl_26</td>\n",
       "      <td>nl_3</td>\n",
       "      <td>nl_44</td>\n",
       "      <td>nl_7</td>\n",
       "      <td>nl_32</td>\n",
       "      <td>nl_42</td>\n",
       "      <td>nl_43</td>\n",
       "      <td>nl_30</td>\n",
       "      <td>nl_24</td>\n",
       "      <td>a</td>\n",
       "      <td>b</td>\n",
       "      <td>a</td>\n",
       "      <td>b</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>a</td>\n",
       "      <td>nl_12</td>\n",
       "      <td>nl_1</td>\n",
       "      <td>nl_16</td>\n",
       "      <td>nl_18</td>\n",
       "      <td>nl_26</td>\n",
       "      <td>nl_28</td>\n",
       "      <td>nl_20</td>\n",
       "      <td>nl_10</td>\n",
       "      <td>nl_39</td>\n",
       "      <td>nl_7</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>a</td>\n",
       "      <td>nl_16</td>\n",
       "      <td>nl_20</td>\n",
       "      <td>nl_5</td>\n",
       "      <td>nl_4</td>\n",
       "      <td>nl_0</td>\n",
       "      <td>nl_28</td>\n",
       "      <td>nl_34</td>\n",
       "      <td>nl_47</td>\n",
       "      <td>nl_6</td>\n",
       "      <td>nl_31</td>\n",
       "      <td>a</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>a</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   const_col noise_0 noise_1 noise_2 noise_3 noise_4 noise_5 noise_6 noise_7  \\\n",
       "0          a   nl_46   nl_41   nl_21   nl_49   nl_16   nl_17   nl_42   nl_16   \n",
       "1          a   nl_16   nl_27    nl_0    nl_5   nl_41    nl_5   nl_40   nl_23   \n",
       "2          a    nl_9   nl_34    nl_8   nl_35    nl_0   nl_24   nl_46   nl_35   \n",
       "3          a   nl_40   nl_31   nl_25   nl_35   nl_37   nl_19   nl_20   nl_46   \n",
       "4          a   nl_16   nl_35   nl_45   nl_28   nl_27   nl_44    nl_1   nl_12   \n",
       "..       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "95         a   nl_24    nl_2    nl_4   nl_12   nl_44    nl_8    nl_0    nl_7   \n",
       "96         a    nl_9   nl_10   nl_17   nl_39    nl_2   nl_31    nl_4   nl_24   \n",
       "97         a    nl_9   nl_26    nl_3   nl_44    nl_7   nl_32   nl_42   nl_43   \n",
       "98         a   nl_12    nl_1   nl_16   nl_18   nl_26   nl_28   nl_20   nl_10   \n",
       "99         a   nl_16   nl_20    nl_5    nl_4    nl_0   nl_28   nl_34   nl_47   \n",
       "\n",
       "   noise_8 noise_9 signal_0 signal_1 signal_2 signal_3 signal_4  \n",
       "0    nl_34    nl_9        a        b        a        b        a  \n",
       "1    nl_27   nl_42        a        a        a        b        b  \n",
       "2    nl_28   nl_25        a        a        b        a        a  \n",
       "3    nl_38    nl_9        b        a        a        a        b  \n",
       "4    nl_18   nl_31        b        b        a        a        a  \n",
       "..     ...     ...      ...      ...      ...      ...      ...  \n",
       "95   nl_48   nl_47        b        b        b        a        a  \n",
       "96   nl_12    nl_6        b        a        a        a        a  \n",
       "97   nl_30   nl_24        a        b        a        b        a  \n",
       "98   nl_39    nl_7        b        b        b        a        a  \n",
       "99    nl_6   nl_31        a        b        b        a        b  \n",
       "\n",
       "[100 rows x 16 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_example_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.092855\n",
       "1     0.702124\n",
       "2     0.624716\n",
       "3    -0.057869\n",
       "4     2.984870\n",
       "        ...   \n",
       "95   -0.903519\n",
       "96    2.625148\n",
       "97    1.778676\n",
       "98   -1.845954\n",
       "99   -1.480355\n",
       "Length: 100, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_example_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will use `vtreat` to impact code all the variables with cross-methods, because we want to look at some extra information. Instructions for using `vtreat` for regression can be found [here](https://github.com/WinVector/pyvtreat/blob/master/Examples/Regression/Regression.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noise_0_impact_code</th>\n",
       "      <th>noise_1_impact_code</th>\n",
       "      <th>noise_2_impact_code</th>\n",
       "      <th>noise_3_impact_code</th>\n",
       "      <th>noise_4_impact_code</th>\n",
       "      <th>noise_5_impact_code</th>\n",
       "      <th>noise_6_impact_code</th>\n",
       "      <th>noise_7_impact_code</th>\n",
       "      <th>noise_8_impact_code</th>\n",
       "      <th>noise_9_impact_code</th>\n",
       "      <th>signal_0_impact_code</th>\n",
       "      <th>signal_1_impact_code</th>\n",
       "      <th>signal_2_impact_code</th>\n",
       "      <th>signal_3_impact_code</th>\n",
       "      <th>signal_4_impact_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.775558e-17</td>\n",
       "      <td>-0.082937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.775558e-17</td>\n",
       "      <td>8.957836e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.860668e+00</td>\n",
       "      <td>1.227627e-01</td>\n",
       "      <td>0.553686</td>\n",
       "      <td>0.734911</td>\n",
       "      <td>-1.195284</td>\n",
       "      <td>0.518659</td>\n",
       "      <td>-1.418944</td>\n",
       "      <td>0.609915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.126731e-01</td>\n",
       "      <td>-0.479059</td>\n",
       "      <td>-0.432493</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.154976e-01</td>\n",
       "      <td>-0.469978</td>\n",
       "      <td>6.394952e-03</td>\n",
       "      <td>2.775558e-17</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.724223</td>\n",
       "      <td>1.426430</td>\n",
       "      <td>0.502723</td>\n",
       "      <td>-1.277764</td>\n",
       "      <td>-0.478077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.076853e+00</td>\n",
       "      <td>-1.252405</td>\n",
       "      <td>-0.050814</td>\n",
       "      <td>-5.230413e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.061539</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.535319</td>\n",
       "      <td>1.377291</td>\n",
       "      <td>-0.862579</td>\n",
       "      <td>1.095629</td>\n",
       "      <td>0.542649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.230413e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.117627e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.431014e+00</td>\n",
       "      <td>-0.486062</td>\n",
       "      <td>-0.811250</td>\n",
       "      <td>1.377291</td>\n",
       "      <td>0.572315</td>\n",
       "      <td>1.095629</td>\n",
       "      <td>-0.511667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4.668268e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.328000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.083165e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.551115e-17</td>\n",
       "      <td>-5.551115e-17</td>\n",
       "      <td>1.240756e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.954283</td>\n",
       "      <td>-1.351057</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>1.220950</td>\n",
       "      <td>0.691481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.031278e-01</td>\n",
       "      <td>-3.880526e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.811250</td>\n",
       "      <td>-1.233949</td>\n",
       "      <td>-0.862579</td>\n",
       "      <td>1.095629</td>\n",
       "      <td>0.542649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>7.325165e-01</td>\n",
       "      <td>-0.085311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.775558e-17</td>\n",
       "      <td>2.775558e-17</td>\n",
       "      <td>1.637958</td>\n",
       "      <td>-1.032072e+00</td>\n",
       "      <td>2.775558e-17</td>\n",
       "      <td>-2.775558e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.087247</td>\n",
       "      <td>1.340127</td>\n",
       "      <td>0.518659</td>\n",
       "      <td>1.178257</td>\n",
       "      <td>0.609915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>7.325165e-01</td>\n",
       "      <td>0.332794</td>\n",
       "      <td>-1.521902</td>\n",
       "      <td>2.775558e-17</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.775558e-17</td>\n",
       "      <td>-0.009648</td>\n",
       "      <td>0.734911</td>\n",
       "      <td>-1.195284</td>\n",
       "      <td>0.518659</td>\n",
       "      <td>-1.418944</td>\n",
       "      <td>0.609915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.767260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.134287e+00</td>\n",
       "      <td>-0.238486</td>\n",
       "      <td>-2.246184e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.202360</td>\n",
       "      <td>-1.102484</td>\n",
       "      <td>-0.690518</td>\n",
       "      <td>1.070849</td>\n",
       "      <td>0.565276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-4.668268e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.947170</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-4.546943e-01</td>\n",
       "      <td>-0.339889</td>\n",
       "      <td>-5.551115e-17</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-6.753481e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.708476</td>\n",
       "      <td>-1.351057</td>\n",
       "      <td>-0.594712</td>\n",
       "      <td>1.220950</td>\n",
       "      <td>-0.630371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    noise_0_impact_code  noise_1_impact_code  noise_2_impact_code  \\\n",
       "0         -2.775558e-17            -0.082937             0.000000   \n",
       "1         -2.126731e-01            -0.479059            -0.432493   \n",
       "2          2.076853e+00            -1.252405            -0.050814   \n",
       "3          0.000000e+00             0.000000             0.000000   \n",
       "4         -4.668268e-01             0.000000             1.328000   \n",
       "..                  ...                  ...                  ...   \n",
       "95         0.000000e+00             0.000000             0.000000   \n",
       "96         7.325165e-01            -0.085311             0.000000   \n",
       "97         7.325165e-01             0.332794            -1.521902   \n",
       "98         0.000000e+00             0.767260             0.000000   \n",
       "99        -4.668268e-01             0.000000             0.947170   \n",
       "\n",
       "    noise_3_impact_code  noise_4_impact_code  noise_5_impact_code  \\\n",
       "0          2.775558e-17         8.957836e-01             0.000000   \n",
       "1          0.000000e+00        -1.154976e-01            -0.469978   \n",
       "2         -5.230413e-01         0.000000e+00             0.061539   \n",
       "3         -5.230413e-01         0.000000e+00             0.000000   \n",
       "4          0.000000e+00        -1.083165e+00             0.000000   \n",
       "..                  ...                  ...                  ...   \n",
       "95         4.031278e-01        -3.880526e-01             0.000000   \n",
       "96         2.775558e-17         2.775558e-17             1.637958   \n",
       "97         2.775558e-17         0.000000e+00             0.000000   \n",
       "98         0.000000e+00         1.134287e+00            -0.238486   \n",
       "99         0.000000e+00        -4.546943e-01            -0.339889   \n",
       "\n",
       "    noise_6_impact_code  noise_7_impact_code  noise_8_impact_code  \\\n",
       "0          0.000000e+00        -2.860668e+00         1.227627e-01   \n",
       "1          6.394952e-03         2.775558e-17         0.000000e+00   \n",
       "2          0.000000e+00         0.000000e+00         0.000000e+00   \n",
       "3          7.117627e-02         0.000000e+00        -1.431014e+00   \n",
       "4         -5.551115e-17        -5.551115e-17         1.240756e+00   \n",
       "..                  ...                  ...                  ...   \n",
       "95         0.000000e+00         0.000000e+00         0.000000e+00   \n",
       "96        -1.032072e+00         2.775558e-17        -2.775558e-17   \n",
       "97         0.000000e+00         0.000000e+00        -2.775558e-17   \n",
       "98        -2.246184e-01         0.000000e+00         0.000000e+00   \n",
       "99        -5.551115e-17         0.000000e+00        -6.753481e-01   \n",
       "\n",
       "    noise_9_impact_code  signal_0_impact_code  signal_1_impact_code  \\\n",
       "0              0.553686              0.734911             -1.195284   \n",
       "1              0.000000              0.724223              1.426430   \n",
       "2              0.000000              0.535319              1.377291   \n",
       "3             -0.486062             -0.811250              1.377291   \n",
       "4              0.000000             -0.954283             -1.351057   \n",
       "..                  ...                   ...                   ...   \n",
       "95             0.000000             -0.811250             -1.233949   \n",
       "96             0.000000             -1.087247              1.340127   \n",
       "97            -0.009648              0.734911             -1.195284   \n",
       "98             0.000000             -1.202360             -1.102484   \n",
       "99             0.000000              0.708476             -1.351057   \n",
       "\n",
       "    signal_2_impact_code  signal_3_impact_code  signal_4_impact_code  \n",
       "0               0.518659             -1.418944              0.609915  \n",
       "1               0.502723             -1.277764             -0.478077  \n",
       "2              -0.862579              1.095629              0.542649  \n",
       "3               0.572315              1.095629             -0.511667  \n",
       "4               0.401000              1.220950              0.691481  \n",
       "..                   ...                   ...                   ...  \n",
       "95             -0.862579              1.095629              0.542649  \n",
       "96              0.518659              1.178257              0.609915  \n",
       "97              0.518659             -1.418944              0.609915  \n",
       "98             -0.690518              1.070849              0.565276  \n",
       "99             -0.594712              1.220950             -0.630371  \n",
       "\n",
       "[100 rows x 15 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vtreat_coder = vtreat.NumericOutcomeTreatment(\n",
    "    outcome_name = None,\n",
    "    params = vtreat.vtreat_parameters({\n",
    "        'coders': {'impact_code'},\n",
    "        'filter_to_recommended': False\n",
    "    }))\n",
    "vtreat_cross_frame = vtreat_coder.fit_transform(d_example_s, y_example_s)\n",
    "\n",
    "# the frame of cross-validated encoded variables\n",
    "vtreat_cross_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have deliberately turned off `vtreat`'s feature pruning to allow the noise columns in, to demonstrate overfitting. `vtreat` itself has out-of-sample significance estimates which allow for reliable feature pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['noise_0_impact_code', 'noise_1_impact_code', 'noise_2_impact_code',\n",
       "       'noise_3_impact_code', 'noise_4_impact_code', 'noise_5_impact_code',\n",
       "       'noise_6_impact_code', 'noise_7_impact_code', 'noise_8_impact_code',\n",
       "       'noise_9_impact_code', 'signal_0_impact_code', 'signal_1_impact_code',\n",
       "       'signal_2_impact_code', 'signal_3_impact_code', 'signal_4_impact_code'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vtreat_cross_frame.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice not all columns were coded; in particular, `vtreat` eliminates constant columns.  The columns considered are reported in `vtreat_coder.score_frame_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>orig_variable</th>\n",
       "      <th>treatment</th>\n",
       "      <th>R2</th>\n",
       "      <th>significance</th>\n",
       "      <th>default_threshold</th>\n",
       "      <th>recommended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>noise_0_impact_code</td>\n",
       "      <td>noise_0</td>\n",
       "      <td>impact_code</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>9.840082e-01</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>noise_1_impact_code</td>\n",
       "      <td>noise_1</td>\n",
       "      <td>impact_code</td>\n",
       "      <td>0.014053</td>\n",
       "      <td>2.401227e-01</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>noise_2_impact_code</td>\n",
       "      <td>noise_2</td>\n",
       "      <td>impact_code</td>\n",
       "      <td>0.010295</td>\n",
       "      <td>3.151458e-01</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>noise_3_impact_code</td>\n",
       "      <td>noise_3</td>\n",
       "      <td>impact_code</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>7.973835e-01</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>noise_4_impact_code</td>\n",
       "      <td>noise_4</td>\n",
       "      <td>impact_code</td>\n",
       "      <td>0.061040</td>\n",
       "      <td>1.320812e-02</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>noise_5_impact_code</td>\n",
       "      <td>noise_5</td>\n",
       "      <td>impact_code</td>\n",
       "      <td>0.003886</td>\n",
       "      <td>5.377935e-01</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>noise_6_impact_code</td>\n",
       "      <td>noise_6</td>\n",
       "      <td>impact_code</td>\n",
       "      <td>0.016734</td>\n",
       "      <td>1.995865e-01</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>noise_7_impact_code</td>\n",
       "      <td>noise_7</td>\n",
       "      <td>impact_code</td>\n",
       "      <td>0.003024</td>\n",
       "      <td>5.868620e-01</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>noise_8_impact_code</td>\n",
       "      <td>noise_8</td>\n",
       "      <td>impact_code</td>\n",
       "      <td>0.050362</td>\n",
       "      <td>2.479017e-02</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>noise_9_impact_code</td>\n",
       "      <td>noise_9</td>\n",
       "      <td>impact_code</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>9.619327e-01</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>signal_0_impact_code</td>\n",
       "      <td>signal_0</td>\n",
       "      <td>impact_code</td>\n",
       "      <td>0.111925</td>\n",
       "      <td>6.688370e-04</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>signal_1_impact_code</td>\n",
       "      <td>signal_1</td>\n",
       "      <td>impact_code</td>\n",
       "      <td>0.273179</td>\n",
       "      <td>2.432494e-08</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>signal_2_impact_code</td>\n",
       "      <td>signal_2</td>\n",
       "      <td>impact_code</td>\n",
       "      <td>0.078100</td>\n",
       "      <td>4.864999e-03</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>signal_3_impact_code</td>\n",
       "      <td>signal_3</td>\n",
       "      <td>impact_code</td>\n",
       "      <td>0.263524</td>\n",
       "      <td>4.722634e-08</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>signal_4_impact_code</td>\n",
       "      <td>signal_4</td>\n",
       "      <td>impact_code</td>\n",
       "      <td>0.065905</td>\n",
       "      <td>9.929392e-03</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                variable orig_variable    treatment        R2  significance  \\\n",
       "0    noise_0_impact_code       noise_0  impact_code  0.000004  9.840082e-01   \n",
       "1    noise_1_impact_code       noise_1  impact_code  0.014053  2.401227e-01   \n",
       "2    noise_2_impact_code       noise_2  impact_code  0.010295  3.151458e-01   \n",
       "3    noise_3_impact_code       noise_3  impact_code  0.000676  7.973835e-01   \n",
       "4    noise_4_impact_code       noise_4  impact_code  0.061040  1.320812e-02   \n",
       "5    noise_5_impact_code       noise_5  impact_code  0.003886  5.377935e-01   \n",
       "6    noise_6_impact_code       noise_6  impact_code  0.016734  1.995865e-01   \n",
       "7    noise_7_impact_code       noise_7  impact_code  0.003024  5.868620e-01   \n",
       "8    noise_8_impact_code       noise_8  impact_code  0.050362  2.479017e-02   \n",
       "9    noise_9_impact_code       noise_9  impact_code  0.000023  9.619327e-01   \n",
       "10  signal_0_impact_code      signal_0  impact_code  0.111925  6.688370e-04   \n",
       "11  signal_1_impact_code      signal_1  impact_code  0.273179  2.432494e-08   \n",
       "12  signal_2_impact_code      signal_2  impact_code  0.078100  4.864999e-03   \n",
       "13  signal_3_impact_code      signal_3  impact_code  0.263524  4.722634e-08   \n",
       "14  signal_4_impact_code      signal_4  impact_code  0.065905  9.929392e-03   \n",
       "\n",
       "    default_threshold  recommended  \n",
       "0            0.066667        False  \n",
       "1            0.066667        False  \n",
       "2            0.066667        False  \n",
       "3            0.066667        False  \n",
       "4            0.066667        False  \n",
       "5            0.066667        False  \n",
       "6            0.066667        False  \n",
       "7            0.066667        False  \n",
       "8            0.066667         True  \n",
       "9            0.066667        False  \n",
       "10           0.066667         True  \n",
       "11           0.066667         True  \n",
       "12           0.066667         True  \n",
       "13           0.066667         True  \n",
       "14           0.066667         True  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_show = ['variable', 'orig_variable', 'treatment', 'R2', \n",
    "                'significance', 'default_threshold', 'recommended']\n",
    "\n",
    "vtreat_coder.score_frame_.loc[:, cols_to_show]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above summary shows the R-squared (`R2`) of each variable when considered as a one-variable model for `y_example_s`, as well as the estimated significance (`significance`) of that fit. The R-squareds and significances are computed in a cross-validated manner, so they are good estimates of future out-of-sample performance.\n",
    "\n",
    "The `recommended` column marks which variables have significances below the `default_threshold`, which itself is chosen to allow at most one uninformative column through (in expectation). In this case, we see that all the signal variables and only one noise variable are recommended.\n",
    "\n",
    "From the score frame, we can examine the distribution of cross-validated significances grouped by whether the variable was a signal variable or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "str_strip = re.compile('_.*$')\n",
    "sig_frame = pandas.DataFrame({\n",
    "    'significance': vtreat_coder.score_frame_['significance'],\n",
    "    'variable_type': [str_strip.sub('', v) for v in \n",
    "                      vtreat_coder.score_frame_['variable']]\n",
    "    })\n",
    "splt = seaborn.kdeplot(\n",
    "    sig_frame['significance'][sig_frame['variable_type'] == 'noise'], \n",
    "    color=\"r\",\n",
    "    shade=True,\n",
    "    label='noise significance')\n",
    "splt = seaborn.kdeplot(\n",
    "    sig_frame['significance'][sig_frame['variable_type'] == 'signal'], \n",
    "    color=\"b\",\n",
    "    shade=True,\n",
    "    label='signal significance')\n",
    "splt.set(xlim=(0,1))\n",
    "splt.set_yscale('log')\n",
    "_ =splt.set_title('distribution of training R2 grouped by variable type\\n(log y scale)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is as we would hope: the signaling variables have p-values concentrated near zero, and the noise variables have significances that are uniformly distributed in the interval `[0, 1]`.\n",
    "\n",
    "Now we try fitting a linear model using the encoded frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.840</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.811</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   29.31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 10 Mar 2020</td> <th>  Prob (F-statistic):</th> <td>4.52e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:01:40</td>     <th>  Log-Likelihood:    </th> <td> -136.14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   304.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    84</td>      <th>  BIC:               </th> <td>   346.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    15</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.1934</td> <td>    0.120</td> <td>    1.608</td> <td> 0.112</td> <td>   -0.046</td> <td>    0.433</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -0.1831</td> <td>    0.167</td> <td>   -1.096</td> <td> 0.276</td> <td>   -0.515</td> <td>    0.149</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.2513</td> <td>    0.208</td> <td>    1.210</td> <td> 0.230</td> <td>   -0.162</td> <td>    0.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.0320</td> <td>    0.159</td> <td>    0.202</td> <td> 0.841</td> <td>   -0.283</td> <td>    0.347</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.1212</td> <td>    0.210</td> <td>    0.577</td> <td> 0.565</td> <td>   -0.296</td> <td>    0.539</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   -0.1123</td> <td>    0.186</td> <td>   -0.604</td> <td> 0.547</td> <td>   -0.482</td> <td>    0.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>   -0.3233</td> <td>    0.165</td> <td>   -1.961</td> <td> 0.053</td> <td>   -0.651</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>   -0.0417</td> <td>    0.162</td> <td>   -0.258</td> <td> 0.797</td> <td>   -0.363</td> <td>    0.280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>    0.2390</td> <td>    0.169</td> <td>    1.417</td> <td> 0.160</td> <td>   -0.096</td> <td>    0.574</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    0.1610</td> <td>    0.149</td> <td>    1.083</td> <td> 0.282</td> <td>   -0.135</td> <td>    0.457</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>   -0.1025</td> <td>    0.198</td> <td>   -0.517</td> <td> 0.606</td> <td>   -0.496</td> <td>    0.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>    1.1851</td> <td>    0.135</td> <td>    8.756</td> <td> 0.000</td> <td>    0.916</td> <td>    1.454</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>    0.8837</td> <td>    0.092</td> <td>    9.631</td> <td> 0.000</td> <td>    0.701</td> <td>    1.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>    1.4632</td> <td>    0.188</td> <td>    7.780</td> <td> 0.000</td> <td>    1.089</td> <td>    1.837</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>    0.8204</td> <td>    0.094</td> <td>    8.730</td> <td> 0.000</td> <td>    0.634</td> <td>    1.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>    1.8506</td> <td>    0.212</td> <td>    8.725</td> <td> 0.000</td> <td>    1.429</td> <td>    2.272</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.383</td> <th>  Durbin-Watson:     </th> <td>   2.156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.304</td> <th>  Jarque-Bera (JB):  </th> <td>   1.621</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.057</td> <th>  Prob(JB):          </th> <td>   0.445</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.387</td> <th>  Cond. No.          </th> <td>    3.34</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.840\n",
       "Model:                            OLS   Adj. R-squared:                  0.811\n",
       "Method:                 Least Squares   F-statistic:                     29.31\n",
       "Date:                Tue, 10 Mar 2020   Prob (F-statistic):           4.52e-27\n",
       "Time:                        18:01:40   Log-Likelihood:                -136.14\n",
       "No. Observations:                 100   AIC:                             304.3\n",
       "Df Residuals:                      84   BIC:                             346.0\n",
       "Df Model:                          15                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.1934      0.120      1.608      0.112      -0.046       0.433\n",
       "x1            -0.1831      0.167     -1.096      0.276      -0.515       0.149\n",
       "x2             0.2513      0.208      1.210      0.230      -0.162       0.664\n",
       "x3             0.0320      0.159      0.202      0.841      -0.283       0.347\n",
       "x4             0.1212      0.210      0.577      0.565      -0.296       0.539\n",
       "x5            -0.1123      0.186     -0.604      0.547      -0.482       0.257\n",
       "x6            -0.3233      0.165     -1.961      0.053      -0.651       0.005\n",
       "x7            -0.0417      0.162     -0.258      0.797      -0.363       0.280\n",
       "x8             0.2390      0.169      1.417      0.160      -0.096       0.574\n",
       "x9             0.1610      0.149      1.083      0.282      -0.135       0.457\n",
       "x10           -0.1025      0.198     -0.517      0.606      -0.496       0.292\n",
       "x11            1.1851      0.135      8.756      0.000       0.916       1.454\n",
       "x12            0.8837      0.092      9.631      0.000       0.701       1.066\n",
       "x13            1.4632      0.188      7.780      0.000       1.089       1.837\n",
       "x14            0.8204      0.094      8.730      0.000       0.634       1.007\n",
       "x15            1.8506      0.212      8.725      0.000       1.429       2.272\n",
       "==============================================================================\n",
       "Omnibus:                        2.383   Durbin-Watson:                   2.156\n",
       "Prob(Omnibus):                  0.304   Jarque-Bera (JB):                1.621\n",
       "Skew:                           0.057   Prob(JB):                        0.445\n",
       "Kurtosis:                       2.387   Cond. No.                         3.34\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_model = statsmodels.api.OLS(\n",
    "    y_example_s, \n",
    "    statsmodels.api.add_constant(\n",
    "        vtreat_cross_frame.values, \n",
    "        has_constant='add'))\n",
    "good_fit = good_model.fit()\n",
    "\n",
    "train_r2 = sklearn.metrics.r2_score(\n",
    "    y_true=y_example_s, \n",
    "    y_pred=good_fit.predict(\n",
    "        statsmodels.api.add_constant(\n",
    "            vtreat_cross_frame.values, \n",
    "            has_constant='add')))\n",
    "assert train_r2 > 0.7\n",
    "\n",
    "good_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8396086797764389"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.r2_score(\n",
    "    y_true=y_example_s, \n",
    "    y_pred=good_fit.predict(\n",
    "        statsmodels.api.add_constant(\n",
    "            vtreat_cross_frame.values, \n",
    "            has_constant='add')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear model has a reasonable, but not outrageous, R-squared on the training data, and correctly estimates that the signal variables are more significant than the noise variables. \n",
    "\n",
    "More importantly, this model performs about the same on fresh identically distributed data. We can show this by generating test sets distributed identically to the training set, evaluating them with the model, and estimating R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def f():\n",
    "    d_test_s, y_test_s = mk_data(\n",
    "        nrow=100,\n",
    "        n_noise_var=10,\n",
    "        n_noise_level=50,\n",
    "        n_signal_var=5)\n",
    "\n",
    "    vtreat_test_frame = vtreat_coder.transform(d_test_s)\n",
    "\n",
    "    return sklearn.metrics.r2_score(\n",
    "                y_true=y_test_s, \n",
    "                y_pred=good_fit.predict(\n",
    "                    statsmodels.api.add_constant(\n",
    "                        vtreat_test_frame.values, \n",
    "                        has_constant='add')))\n",
    "\n",
    "# the array of R-squared for the repeated tests\n",
    "test_r2 = numpy.asarray([f() for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert numpy.mean(test_r2) >= train_r2 - 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "splot = seaborn.distplot(test_r2)\n",
    "_ = splot.set_title('distribution of test R2 under repeated draws')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect that `category_encoders.target_encoder.TargetEncoder` would also handle the above example correctly.  And we could write code to break `vtreat`, just as we wrote code to break `category_encoders.target_encoder.TargetEncoder`. \n",
    "\n",
    "The point is `vtreat` is easy to use, and supplies a wide variety of safe and useful variable transforms for predictive modeling and machine learning. \n",
    "\n",
    "## So Should We Use Cross-Methods? Or Not?\n",
    "\n",
    "In this article, we've shown that cross-methods *do* leak information about the training data, so it is not as \"safe\" in that sense as splitting training data into multiple partitions: one for setting parameters or calculating data transformations, one for training the model, and one for evaluating it. So if you have a large enough data set, partitioning it is probably preferable to cross-methods.\n",
    "We've also seen that deterministic cross-method schemes like leave-one-out are particularly leaky.\n",
    "\n",
    "On the other hand, we've also seen that the leak from randomized cross-methods is small enough that linear regression does not seem to see it: the linear models fit to randomized cross-validated encodings above correctly identified noise variables as uninformative most of the time. From experience, we (the authors) have seen that tree ensemble methods like random forest and gradient boosted trees also do not seem too sensitive to the leak ([here's an `xgboost` example](https://github.com/WinVector/pyvtreat/blob/master/Examples/KDD2009Example/KDD2009Example_no_filter.ipynb)).\n",
    "\n",
    "So what we seem to be able to say is that cross-methods lower the information leak enough that the transformed training data appears safe to use with a reasonable downstream modeling algorithm. This is consistent with the results from superlearning and stacking, which use cross-methods to build \"features\" corresponding to the individual sub-learners, and then fit a model from these features to learn the overall ensemble model. Just as sometimes it is appropriate to introduce a bit of bias to for a large reduction in variance (bias/variance trade-off), it can be appropriate to pursue a favorable leak/variance trade-off.\n",
    "\n",
    "It's worth noting that the recommended method to combine the sub-learners in stacking is non-negative linear regression, which is essentially a form of regularized regression. While [regularization is no substitute for cross-methods](http://www.win-vector.com/blog/2019/11/when-cross-validation-is-more-powerful-than-regularization/), it can certainly help reduce the possibility of overfit. As we saw above, linear regression *is* sensitive to the leave-one-out leak, because it can use large coefficients to multiply the leak's magnitude. Regularization would help prevent that, and non-negativity constraints completely eliminate the leak that we demonstrated, as that leak requires negative coefficients. So properly cross-validated encodings are typically \"safe\" at least when used with regularized regression.\n",
    "\n",
    "What about tree ensemble methods? Random forest and gradient boosting are higher complexity models, so there is more risk that they might decode the leak. Our speculation is that the averaging inherent in random forest may serve as a \"regularization\" or smoothing step that helps mitigate this risk; and of course limiting the depth of the trees in a tree ensemble method is also a form of regularization.\n",
    "\n",
    "### Even hold-out sets can be leaky!\n",
    "\n",
    "However, re-using a cross-validated set multiple times within the model fitting process, for example with stepwise regression, or using cross-methods for multiple layers of nested models, probably increases the chance that the modeling algorithm will decode the leak. In fact, even hold-out sets leak information when used this way!\n",
    "\n",
    "The last section of [this article](http://www.win-vector.com/blog/2015/10/a-simpler-explanation-of-differential-privacy/) shows an example of hold-out set leakage during stepwise regression. And [here](http://proceedings.mlr.press/v37/blum15.pdf) is an example of leaderboard hold-out set leakage during Kaggle competitions. The leakage in both these situations occurs because the hold-out set is used multiple times during the model fitting/model tuning/model selection process, and hence leaks information that leads to model overfit.\n",
    "\n",
    "Model selection is *not* an unbiased procedure, and the bias, however small, can lead to information leakage. For any sort of hyper-parameter tuning or model search, the procedure is biased (though likely of small magnitude) no matter what hold-out procedure we use. Therefore, avoiding cross-validation leakage isn't the only problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Considerations\n",
    "\n",
    "As we mentioned above, `vtreat` encodes high-cardinality categorical variables by their conditional difference from mean outcome (impact coding), rather than the conditional mean (target coding). Though the proof is out of scope of this article, impact coding has slightly lower variance, and a slightly lower magnitude leak for randomized cross-methods. However, the difference isn't very important in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "We can summarize the takeaways from the experiments that we've shown here:\n",
    "\n",
    "* If you have enough training data, partitioning it into sets for data transformations, model training, and evaluation may be preferable to cross-methods.\n",
    "* If partitioning is not an option, cross methods may be good enough for reasonable applications.\n",
    "* Avoid leave-one-out and other deterministic cross-method schemes.\n",
    "* When using cross-validated encoded data, prefer regularized methods for the downstream model fitting when possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
